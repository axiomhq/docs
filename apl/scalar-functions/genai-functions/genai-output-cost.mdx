---
title: 'genai_output_cost'
description: 'This page explains how to use the genai_output_cost function in APL.'
---

The `genai_output_cost` function calculates the cost of output tokens (completion tokens) for a GenAI API call based on the model name and number of output tokens. This helps you understand and track the cost of generated responses separately from prompts.

You can use this function to analyze generation costs, optimize response length for cost efficiency, track output spending separately, or create detailed cost breakdowns.

## For users of other query languages

If you come from other query languages, this section explains how to adjust your existing queries to achieve the same results in APL.

<AccordionGroup>
<Accordion title="Splunk SPL users">

In Splunk SPL, you would need to lookup pricing and calculate costs manually.

<CodeGroup>
```sql Splunk example
| lookup model_pricing model OUTPUT output_price
| eval output_cost=(output_tokens * output_price / 1000000)
```

```kusto APL equivalent
['ai-logs']
| extend output_cost = genai_output_cost(model, output_tokens)
```
</CodeGroup>

</Accordion>
<Accordion title="ANSI SQL users">

In ANSI SQL, you would join with a pricing table and calculate output costs.

<CodeGroup>
```sql SQL example
SELECT 
  l.*,
  (l.output_tokens * p.output_price / 1000000) as output_cost
FROM ai_logs l
JOIN model_pricing p ON l.model = p.model_name
```

```kusto APL equivalent
['ai-logs']
| extend output_cost = genai_output_cost(model, output_tokens)
```
</CodeGroup>

</Accordion>
</AccordionGroup>

## Usage

### Syntax

```kusto
genai_output_cost(model, output_tokens)
```

### Parameters

- **model** (string, required): The name of the AI model (for example, 'gpt-4', 'claude-3-opus', 'gpt-3.5-turbo').
- **output_tokens** (long, required): The number of output tokens (completion tokens) generated by the API call.

### Returns

Returns a real number representing the cost in dollars (USD) for the output tokens based on the model's pricing.

## Use case examples

<Tabs>
<Tab title="Log analysis">

Analyze output costs to understand how much you spend on AI-generated responses versus input prompts.

**Query**

```kusto
['sample-http-logs']
| where uri contains '/api/openai'
| extend model_name = tostring(todynamic(response_body)['model'])
| extend completion_tokens = tolong(todynamic(response_body)['usage']['completion_tokens'])
| extend response_cost = genai_output_cost(model_name, completion_tokens)
| summarize total_output_cost = sum(response_cost) by model_name, bin(_time, 1h)
```

[Run in Playground](https://play.axiom.co/axiom-play-qf1k/query?initForm=%7B%22apl%22%3A%22%5B'sample-http-logs'%5D%20%7C%20where%20uri%20contains%20'%2Fapi%2Fopenai'%20%7C%20extend%20model_name%20%3D%20tostring(todynamic(response_body)%5B'model'%5D)%20%7C%20extend%20completion_tokens%20%3D%20tolong(todynamic(response_body)%5B'usage'%5D%5B'completion_tokens'%5D)%20%7C%20extend%20response_cost%20%3D%20genai_output_cost(model_name%2C%20completion_tokens)%20%7C%20summarize%20total_output_cost%20%3D%20sum(response_cost)%20by%20model_name%2C%20bin(_time%2C%201h)%22%7D)

**Output**

| _time | model_name | total_output_cost |
|-------|------------|-------------------|
| 2024-01-15T10:00:00Z | gpt-4 | 8.92 |
| 2024-01-15T10:00:00Z | gpt-3.5-turbo | 0.45 |

This query breaks down output costs by model and time, showing where generation spending occurs.

</Tab>
<Tab title="OpenTelemetry traces">

Track output costs across different services to identify which services generate expensive responses.

**Query**

```kusto
['otel-demo-traces']
| where ['service.name'] == 'frontend' and kind == 'server'
| extend model = tostring(attributes['ai.model'])
| extend output_tokens = tolong(attributes['ai.completion_tokens'])
| extend generation_cost = genai_output_cost(model, output_tokens)
| summarize total_output_cost = sum(generation_cost), avg_output_cost = avg(generation_cost) by ['service.name']
```

[Run in Playground](https://play.axiom.co/axiom-play-qf1k/query?initForm=%7B%22apl%22%3A%22%5B'otel-demo-traces'%5D%20%7C%20where%20%5B'service.name'%5D%20%3D%3D%20'frontend'%20and%20kind%20%3D%3D%20'server'%20%7C%20extend%20model%20%3D%20tostring(attributes%5B'ai.model'%5D)%20%7C%20extend%20output_tokens%20%3D%20tolong(attributes%5B'ai.completion_tokens'%5D)%20%7C%20extend%20generation_cost%20%3D%20genai_output_cost(model%2C%20output_tokens)%20%7C%20summarize%20total_output_cost%20%3D%20sum(generation_cost)%2C%20avg_output_cost%20%3D%20avg(generation_cost)%20by%20%5B'service.name'%5D%22%7D)

**Output**

| service.name | total_output_cost | avg_output_cost |
|--------------|-------------------|-----------------|
| frontend | 78.34 | 0.0321 |

This query helps you identify which services have the highest output costs and average cost per response.

</Tab>
<Tab title="Security logs">

Monitor for unusually long responses that might indicate abuse or unoptimized usage.

**Query**

```kusto
['sample-http-logs']
| where uri contains '/api/ai'
| extend model = tostring(todynamic(request_body)['model'])
| extend output_tokens = tolong(todynamic(response_body)['usage']['completion_tokens'])
| extend generation_cost = genai_output_cost(model, output_tokens)
| where generation_cost > 1.0
| project _time, id, ['geo.country'], model, output_tokens, generation_cost
```

[Run in Playground](https://play.axiom.co/axiom-play-qf1k/query?initForm=%7B%22apl%22%3A%22%5B'sample-http-logs'%5D%20%7C%20where%20uri%20contains%20'%2Fapi%2Fai'%20%7C%20extend%20model%20%3D%20tostring(todynamic(request_body)%5B'model'%5D)%20%7C%20extend%20output_tokens%20%3D%20tolong(todynamic(response_body)%5B'usage'%5D%5B'completion_tokens'%5D)%20%7C%20extend%20generation_cost%20%3D%20genai_output_cost(model%2C%20output_tokens)%20%7C%20where%20generation_cost%20%3E%201.0%20%7C%20project%20_time%2C%20id%2C%20%5B'geo.country'%5D%2C%20model%2C%20output_tokens%2C%20generation_cost%22%7D)

**Output**

| _time | id | geo.country | model | output_tokens | generation_cost |
|-------|----|--------------|---------| -------------|----------------|
| 2024-01-15T10:30:00Z | user_789 | US | gpt-4 | 18000 | 1.08 |
| 2024-01-15T10:35:00Z | user_234 | RU | gpt-4 | 22000 | 1.32 |

This query identifies responses with unusually high costs, which could indicate verbosity issues, abuse, or opportunities for optimization.

</Tab>
</Tabs>

## List of related functions

- [genai_input_cost](/apl/scalar-functions/genai-functions/genai-input-cost): Calculates input token cost. Use this alongside output costs to understand the full cost breakdown.
- [genai_cost](/apl/scalar-functions/genai-functions/genai-cost): Calculates total cost (input + output). Use this when you need combined costs.
- [genai_get_pricing](/apl/scalar-functions/genai-functions/genai-get-pricing): Gets pricing information. Use this to understand the pricing structure behind cost calculations.
- [genai_extract_assistant_response](/apl/scalar-functions/genai-functions/genai-extract-assistant-response): Extracts the response text. Combine with output costs to analyze cost per response.
- [genai_is_truncated](/apl/scalar-functions/genai-functions/genai-is-truncated): Checks if responses were truncated. Use this to understand if token limits affected output costs.

