---
title: "Observe"
description: "Learn how to observe your deployed AI capabilities in production using Axiom’s AI SDK to capture telemetry."
keywords: ["ai engineering", "rudder", "observe", "telemetry", "withspan", "opentelemetry"]
---

import { Badge } from "/snippets/badge.jsx"
import AIEngineeringInstrumentationSnippet from '/snippets/ai-engineering-instrumentation.mdx'

The **Observe** stage is about understanding how your deployed generative AI capabilities perform in the real world. After creating and evaluating a capability, observing its production behavior is crucial for identifying unexpected issues, tracking costs, and gathering the data needed for future improvements.

## Choose your instrumentation approach

Your observability setup depends on the integration approach you selected in the [Quickstart](/ai-engineering/quickstart):

<CardGroup cols={2}>
  <Card title="Axiom AI SDK" icon="bolt">
    Continue with automatic instrumentation using our wrapper around the Vercel AI SDK. See the section below.
  </Card>
  <Card title="Custom OpenTelemetry" icon="code">
    Send traces using your own tooling following our semantic conventions. Jump to the custom section.
  </Card>
</CardGroup>

## For Axiom AI SDK users

If you followed the [Quickstart](/ai-engineering/quickstart) guide, you should already have OpenTelemetry configured. If not, configure your tracer first:

### Configuring the tracer

You must configure an OTLP trace exporter pointing to your Axiom instance. This is typically done in a dedicated instrumentation file that’s loaded before your app starts. Below is an example in TypeScript.

<AIEngineeringInstrumentationSnippet />

Be sure to define the `AXIOM_TOKEN` and `AXIOM_DATASET` environment variables in your `.env` file.

## Capturing Gen AI telemetry with Axiom’s AI SDK

The foundation of the Observe stage is Axiom’s SDK, which integrates with your app to capture detailed OpenTelemetry traces for every AI interaction.

<Info>
The initial release of Axiom’s AI SDK is focused on providing deep integration with TypeScript applications, particularly those using Vercel’s AI SDK to interact with frontier models.
</Info>

### Instrumenting AI SDK calls

The easiest way to get started is by wrapping your existing AI model client. Axiom’s AI SDK provides helper functions for popular libraries like Vercel’s [AI SDK](https://ai-sdk.dev/docs).

The `wrapAISDKModel` function takes an existing AI model object and returns an instrumented version that will automatically generate trace data for every call.

<CodeGroup>

```typescript OpenAI
// src/shared/openai.ts

import { createOpenAI } from '@ai-sdk/openai';
import { wrapAISDKModel } from 'axiom/ai';

const openaiProvider = createOpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Wrap the model to enable automatic tracing
export const gpt4o = wrapAISDKModel(openaiProvider('gpt-4o'));
export const gpt4oMini = wrapAISDKModel(openaiProvider('gpt-4o-mini'));
```

```typescript Anthropic
// src/shared/anthropic.ts

import { createAnthropic } from '@ai-sdk/anthropic';
import { wrapAISDKModel } from 'axiom/ai';

const anthropicProvider = createAnthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

// Wrap the model to enable automatic tracing
export const claude35Sonnet = wrapAISDKModel(anthropicProvider('claude-3-5-sonnet-20241022'));
export const claude35Haiku = wrapAISDKModel(anthropicProvider('claude-3-5-haiku-20241022'));
```

```typescript Google Gemini
// src/shared/gemini.ts

import { createGoogleGenerativeAI } from '@ai-sdk/google';
import { wrapAISDKModel } from 'axiom/ai';

const geminiProvider = createGoogleGenerativeAI({
  apiKey: process.env.GEMINI_API_KEY,
});

// Wrap the model to enable automatic tracing
export const gemini20Flash = wrapAISDKModel(geminiProvider('gemini-2.0-flash-exp'));
export const gemini15Pro = wrapAISDKModel(geminiProvider('gemini-1.5-pro'));
```

```typescript Grok
// src/shared/grok.ts

import { createXai } from '@ai-sdk/xai';
import { wrapAISDKModel } from 'axiom/ai';

const grokProvider = createXai({
  apiKey: process.env.XAI_API_KEY,
});

// Wrap the model to enable automatic tracing
export const grokBeta = wrapAISDKModel(grokProvider('grok-beta'));
export const grok2Mini = wrapAISDKModel(grokProvider('grok-2-mini'));
```

</CodeGroup>

### Adding context with `withSpan`

While `wrapAISDKModel` handles the automatic instrumentation, the `withSpan` function allows you to add crucial business context to your traces. It creates a parent span around your LLM call and attaches metadata about the `capability` and `step` being executed.

```typescript
// src/app/page.tsx

import { withSpan } from 'axiom/ai';
import { generateText } from 'ai';
import { gpt4o } from '@/shared/openai';

export default async function Page() {
  const userId = 123;

  // Use withSpan to define the capability and step
  const res = await withSpan({ capability: 'get_capital', step: 'generate_answer' }, (span) => {
    // You have access to the OTel span to add custom attributes
    span.setAttribute('user_id', userId);

    return generateText({
      model: gpt4o, // Use the wrapped model
      messages: [
        {
          role: 'user',
          content: 'What is the capital of Spain?',
        },
      ],
    });
  });

  return <p>{res.text}</p>;
}
```

### Instrumenting tool calls with `wrapTool`

For many AI capabilities, the LLM call is only part of the story. If your capability uses tools to interact with external data or services, observing the performance and outcome of those tools is critical. The Axiom AI SDK provides the `wrapTool` and `wrapTools` functions to automatically instrument your Vercel AI SDK tool definitions.

The `wrapTool` helper takes your tool's name and its definition and returns an instrumented version. This wrapper creates a dedicated child span for every tool execution, capturing its arguments, output, and any errors.

```typescript
// src/app/generate-text/page.tsx
import { tool } from 'ai';
import { z } from 'zod';
import { wrapTool } from 'axiom/ai';
import { generateText } from 'ai';
import { gpt4o } from '@/shared/openai';

// In your generateText call, provide wrapped tools
const { text, toolResults } = await generateText({
  model: gpt4o,
  messages: [
    { role: 'system', content: 'You are a helpful assistant.' },
    { role: 'user', content: 'How do I get from Paris to Berlin?' },
  ],
  tools: {
    // Wrap each tool with its name
    findDirections: wrapTool(
      'findDirections', // The name of the tool
      tool({
        description: 'Find directions to a location',
        inputSchema: z.object({
          from: z.string(),
          to: z.string(),
        }),
        execute: async (params) => {
          // Your tool logic here...
          return { directions: `To get from ${params.from} to ${params.to}, use a teleporter.` };
        },
      })
    )
  }
});
```

### Complete instrumentation example

<Accordion title="Full end-to-end code example">
Here's how all three instrumentation functions work together in a single, real-world example:

```typescript
// src/app/page.tsx

import { withSpan, wrapAISDKModel, wrapTool } from 'axiom/ai';
import { generateText, tool } from 'ai';
import { createOpenAI } from '@ai-sdk/openai';
import { z } from 'zod';

// 1. Create and wrap the AI model client
const openaiProvider = createOpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});
const gpt4o = wrapAISDKModel(openaiProvider('gpt-4o'));

// 2. Define and wrap your tool(s)
const findDirectionsTool = wrapTool(
  'findDirections', // The tool name must be passed to the wrapper
  tool({
    description: 'Find directions to a location',
    inputSchema: z.object({ from: z.string(), to: z.string() }),
    execute: async ({ from, to }) => ({
      directions: `To get from ${from} to ${to}, use a teleporter.`,
    }),
  })
);

// 3. In your application logic, use `withSpan` to add context
//    and call the AI model with your wrapped tools.
export default async function Page() {
  const userId = 123;

  const { text } = await withSpan({ capability: 'get_directions', step: 'generate_ai_response' }, async (span) => {
    // You have access to the OTel span to add custom attributes
    span.setAttribute('user_id', userId);

    return generateText({
      model: gpt4o, // Use the wrapped model
      messages: [
        { role: 'system', content: 'You are a helpful assistant.' },
        { role: 'user', content: 'How do I get from Paris to Berlin?' },
      ],
      tools: {
        findDirections: findDirectionsTool, // Use the wrapped tool
      },
    });
  });

  return <p>{text}</p>;
}
```

This demonstrates the three key steps to rich observability:
1. **`wrapAISDKModel`**: Automatically captures telemetry for the LLM provider call
2. **`wrapTool`**: Instruments the tool execution with detailed spans
3. **`withSpan`**: Creates a parent span that ties everything together under a business capability
</Accordion>

## For Custom OpenTelemetry users

If you're using your own instrumentation, send traces directly to Axiom using OpenTelemetry with the required semantic conventions.

See the [Example trace structures](/ai-engineering/semantic-conventions#example-trace-structure) section in our Semantic Conventions guide for some examples.

<Info>
Both SDK and custom OpenTelemetry approaches emit identical `gen_ai.*` attributes, so all the telemetry analysis features below work the same way.
</Info>

## Understanding your AI telemetry

Once instrumented, every LLM call will send a detailed span to your Axiom dataset. These spans are enriched with standardized `gen_ai.*` attributes that make your AI interactions easy to query and analyze.

Key attributes include:

* `gen_ai.capability.name`: The high-level capability name you defined in `withSpan`.
* `gen_ai.step.name`: The specific step within the capability.
* `gen_ai.request.model`: The model requested for the completion.
* `gen_ai.response.model`: The model that actually fulfilled the request.
* `gen_ai.usage.input_tokens`: The number of tokens in the prompt.
* `gen_ai.usage.output_tokens`: The number of tokens in the generated response.
* `gen_ai.prompt`: The full, rendered prompt or message history sent to the model (as a JSON string).
* `gen_ai.completion`: The full response from the model, including tool calls (as a JSON string).
* `gen_ai.response.finish_reasons`: The reason the model stopped generating tokens (e.g., `stop`, `tool-calls`).
* **`gen_ai.tool.name`**: The name of the executed tool.
* **`gen_ai.tool.arguments`**: The arguments passed to the tool (as a JSON string).
* **`gen_ai.tool.message`**: The result returned by the tool (as a JSON string).

## Visualizing traces in the console

Visualizing and making sense of this telemetry data is a core part of the Axiom Console experience.

* A dedicated **AI Trace Waterfall** view will visualize single and multi-step LLM workflows, with clear input/output inspection at each stage.
* A pre-built **Gen AI OTel Dashboard** will automatically appear for any dataset receiving AI telemetry. It will feature elements for tracking cost per invocation, time-to-first-token, call counts by model, and error rates.

## What’s next?

Now that you are capturing and analyzing production telemetry, the next step is to use these insights to improve your capability.

Learn more in the [Iterate](/ai-engineering/iterate) page.