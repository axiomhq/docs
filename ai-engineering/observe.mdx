---
title: "Overview of Observe stage"
description: "Learn how to observe your deployed AI capabilities in production using Axiom AI SDK to capture telemetry."
sidebarTitle: Overview
keywords: ["ai engineering", "observe", "telemetry", "withspan", "opentelemetry"]
---

import { Badge } from "/snippets/badge.jsx"
import AIInstrumentationApproaches from "/snippets/ai-instrumentation-approaches.mdx"

In the Observe stage of the AI engineering lifecycle, the focus is on understanding how your deployed generative AI capabilities perform in the real world. After creating and evaluating a capability, observing its production behavior is crucial for identifying unexpected issues, tracking costs, and gathering the data needed for future improvements.

## Instrument your app

<AIInstrumentationApproaches />

## Visualize traces in Console

Visualizing and making sense of this telemetry data is a core part of the Axiom Console experience:

- A dedicated **AI Trace Waterfall** view visualizes single and multi-step LLM workflows, with clear input/output inspection at each stage.
- A pre-built **Gen AI OTel Dashboard** automatically appears for any dataset receiving AI telemetry. It features elements for tracking cost per invocation, time-to-first-token, call counts by model, and error rates.

## Whatâ€™s next?

After capturing and analyzing production telemetry, use these insights to improve your capability. Learn more in [Iterate](/ai-engineering/iterate).
