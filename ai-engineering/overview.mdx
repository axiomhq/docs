---
title: "AI engineering overview"
description: "Build increasingly sophisticated AI capabilities with confidence using systematic evaluation and observability."
sidebarTitle: Overview
keywords: ["ai engineering", "AI engineering", "prompt engineering", "generative ai"]
tag: "NEW"
---

import { definitions } from '/snippets/definitions.mdx'

Generative AI development is fundamentally different from traditional software engineering. Its outputs are probabilistic, not deterministic. This variability makes it challenging to guarantee quality and predict failure modes without the right infrastructure.

Axiom's AI engineering capabilities build on the foundational <Tooltip tip={definitions.EventDB}>EventDB</Tooltip> and <Tooltip tip={definitions.Console}>Console</Tooltip> to provide systematic evaluation and observability for AI systems. Whether you're building single-turn model interactions, multi-step workflows, or complex multi-agent systems, Axiom helps you push boundaries and ship with confidence.

### Axiom AI engineering workflow

Axiom provides a structured, iterative workflow for developing AI capabilities. The workflow builds statistical confidence in systems that aren’t entirely predictable through systematic evaluation and continuous improvement, from initial prototype to production monitoring.

The core stages are:

* **Create**: Define your AI capability using popular models, prototype it using various models, and gather gather reference examples to establish ground truth for evaluation.
* **Evaluate**: Systematically test your capability’s performance against reference data using custom scorers to measure accuracy, quality, and cost. Use Axiom's evaluation framework to run experiments with different configurations and track improvements over time.
* **Observe**: Deploy your capability and collect rich telemetry on every LLM call and tool execution. Use online evaluations to monitor for performance degradation and discover edge cases in production.
* **Iterate**: Use insights from production monitoring and evaluation results to refine prompts, augment reference datasets, and improve the capability. Run new evaluations to verify improvements before deploying changes.

### What’s next?

* To understand the key terms used in AI engineering, see the [Concepts](/ai-engineering/concepts) page.
* To start building, follow the [Quickstart](/ai-engineering/quickstart) page.