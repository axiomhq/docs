---
title: "Analyze results"
description: "This page explains how to analyze results from online evaluations."
keywords: ["ai engineering", "console", "results", "analysis", "comparison", "baseline", "online evaluations"]
---

The Axiom Console displays results from evaluations. Online evaluation results stream in continuously from production traffic scored by `onlineEval`.

Online evaluation scores appear in the Console alongside your production traces. Each `onlineEval` call creates spans tagged with `eval.tags: ["online"]` so you can filter for them specifically.

### Monitor scorer trends

Use online eval scores to track production quality over time. Look for:

- **Score drops** that correlate with deployments or upstream changes.
- **Scorer disagreement** where heuristic scorers pass but LLM judges flag quality issues, or vice versa.
- **Sampling gaps** where low sampling rates on expensive scorers leave blind spots in coverage.

### Feed insights back to offline evaluations

Online evaluations often surface edge cases that your offline test collections don't cover. When you spot a pattern of failures in production:

1. Add the failing inputs to your offline collection as new test cases with expected outputs.
1. Run offline evaluations to reproduce the issue and verify your fix.
1. Deploy the fix and confirm that online evaluation scores recover.

This feedback loop between online and offline evaluations is where the two approaches reinforce each other.

## What's next?

To iterate on your capability based on evaluation results, see [Iterate](/ai-engineering/iterate).
