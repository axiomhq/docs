---
title: "Measure"
description: "Learn how to measure the quality of your AI capabilities by running evaluations against ground truth data."
keywords: ["ai engineering", "AI engineering", "measure", "evals", "evaluation", "scoring", "scorers", "graders", "scores"]
---

import { Badge } from "/snippets/badge.jsx"
import { definitions } from "/snippets/definitions.mdx"

<Warning>
The evaluation framework described here is in active development. Axiom is working with design partners to shape what’s built. [Contact Axiom](https://www.axiom.co/contact) to get early access and join a focused group of teams shaping these tools.
</Warning>

The **Measure** stage is where you quantify the quality and effectiveness of your AI <Tooltip tip={definitions.Capability}>capability</Tooltip>. Instead of relying on anecdotal checks, this stage uses a systematic process called an <Tooltip tip={definitions.Eval}>eval</Tooltip> to score your capability’s performance against a known set of correct examples (<Tooltip tip={definitions.GroundTruth}>ground truth</Tooltip>). This provides a data-driven benchmark to ensure a capability is ready for production and to track its quality over time.

Evaluations (evals) are systematic tests that measure how well your AI features perform. Instead of manually testing AI outputs, evals automatically run your AI code against test datasets and score the results using custom metrics. This lets you catch regressions, compare different approaches, and confidently improve your AI features over time.

## Initial setup

1. Follow the [Quickstart](/ai-engineering/quickstart) to set up instrumentation for your app.
1. Run the following command to istall the `autoevals` library. This library provides pre-built scorers for common tasks like semantic similarity, factual correctness, and text matching.

    ```bash
    npm install --save-dev autoevals
    ```

### Change instrumentation

Change the instrumentation setup in the `src/instrumentation.ts` file that you have previously created in the [Quickstart](/ai-engineering/quickstart).

```ts /src/instrumentation.ts lines highlight={1,9-10,16-28,58-59}
// Remove `import 'dotenv/config';`
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http';
import { resourceFromAttributes } from '@opentelemetry/resources';
import { SimpleSpanProcessor, NodeTracerProvider } from '@opentelemetry/sdk-trace-node';
import { ATTR_SERVICE_NAME } from '@opentelemetry/semantic-conventions';
import { trace } from "@opentelemetry/api";
import { initAxiomAI, RedactionPolicy } from 'axiom/ai';

// Import the type for the AxiomEvalInstrumentationHook
import type { AxiomEvalInstrumentationHook } from 'axiom/ai/config';

const tracer = trace.getTracer("my-tracer");

let provider: NodeTracerProvider | undefined;

// Wrap your logic in the AxiomEvalInstrumentationHook function
export const setupAppInstrumentation: AxiomEvalInstrumentationHook = async ({
  dataset,
  url,
  token,
}) => {
  if (provider) {
    return { provider };
  }

  if (!dataset || !url || !token) {
    throw new Error('Missing environment variables');
  }

  // Replace the environment variables with the parameters passed to the function
  const exporter = new OTLPTraceExporter({
    url: `${url}/v1/traces`,
    headers: {
      Authorization: `Bearer ${token}`,
      'X-Axiom-Dataset': dataset,
    },
  })

  // Configure the provider to export traces to your Axiom dataset
  provider = new NodeTracerProvider({
    resource: resourceFromAttributes({
      [ATTR_SERVICE_NAME]: 'my-app', // Replace with your service name
    },
    {
      // Use the latest schema version
      // Info: https://opentelemetry.io/docs/specs/semconv/
      schemaUrl: 'https://opentelemetry.io/schemas/1.37.0',
    }),
    spanProcessor: new SimpleSpanProcessor(exporter),
  });

  // Register the provider
  provider.register();

  // Initialize Axiom AI SDK with the configured tracer
  initAxiomAI({ tracer, redactionPolicy: RedactionPolicy.AxiomDefault });

  return { provider };
};
```

### Create Axiom configuration file

At the root of your project, create the Axiom configuration file `/axiom.config.ts`:

```ts /axiom.config.ts
import { defineConfig } from 'axiom/ai/config';
import { setupAppInstrumentation } from './src/instrumentation';

export default defineConfig({
  eval: {
    url: process.env.AXIOM_URL,
    token: process.env.AXIOM_TOKEN,
    dataset: process.env.AXIOM_DATASET,
    
    // Optional: customize which files to run
    include: ['**/*.eval.{ts,js}'],
    
    // Optional: exclude patterns
    exclude: [],
    
    // Optional: timeout for eval execution
    timeoutMs: 60_000,
    
    // Optional: instrumentation hook for OpenTelemetry
    // (created this in the "Create instrumentation setup" step)
    instrumentation: ({ url, token, dataset }) => 
      setupAppInstrumentation({ url, token, dataset }),
  },
});
```

## Write evalulation function

The `Eval` function provides a simple, declarative way to define a test suite for your capability directly in your codebase.

The key parameters of the `Eval` function:

- `data`: An async function that returns your collection of `{ input, expected }` pairs, which serve as your ground truth.
- `task`: The function that executes your AI capability, taking an `input` and producing an `output`.
- `scorers`: An array of scorer functions that score the `output` against the `expected` value.
- `metadata`: Optional metadata for the evaluation, such as a description.

Create an evaluation for a support ticket classification system in the file `/src/evals/ticket-classification.eval.ts`.

```ts /src/evals/ticket-classification.eval.ts expandable
import { experimental_Eval as Eval, Scorer } from 'axiom/ai/evals';
import { generateObject } from 'ai';
import { openai } from '@ai-sdk/openai';
import { wrapAISDKModel } from 'axiom/ai';
import { flag, pickFlags } from '../lib/app-scope';
import { z } from 'zod';
import { ExactMatch } from 'autoevals';

// Define your schemas
const ticketCategorySchema = z.enum(['spam', 'question', 'feature_request', 'bug_report']);
const ticketResponseSchema = z.object({
  category: ticketCategorySchema,
  response: z.string(),
});

// The function you want to evaluate
async function classifyTicket({ subject, content }: { subject?: string; content: string }) {
  const model = flag('ticketClassification.model');
  
  const result = await generateObject({
    model: wrapAISDKModel(openai(model)),
    messages: [
      {
        role: 'system',
        content: `You are a customer support engineer classifying tickets as: spam, question, feature_request, or bug_report.
        
If spam, return a polite auto-close message. Otherwise, say a team member will respond shortly.`,
      },
      {
        role: 'user',
        content: subject ? `Subject: ${subject}\n\n${content}` : content,
      },
    ],
    schema: ticketResponseSchema,
  });

  return result.object;
}

// Custom exact-match scorer
const ExactMatchScorer = Scorer(
  'Exact-Match',
  ({ output, expected }: { output: { response: string }; expected: { response: string } }) => {
    return ExactMatch({
      output: output.response,
      expected: expected.response,
    });
  }
);

// Custom spam classification scorer
const SpamClassificationScorer = Scorer(
  'Spam-Classification',
  ({ output, expected }: { 
    output: { category: string }; 
    expected: { category: string };
  }) => {
    return (expected.category === 'spam') === (output.category === 'spam') ? 1 : 0;
  }
);

// Define the evaluation
Eval('spam-classification', {
  // Specify which flags this eval uses
  configFlags: pickFlags('ticketClassification'),
  
  // Test data with input/expected pairs
  data: () => [
    {
      input: {
        subject: "Congratulations! You've Been Selected for an Exclusive Reward",
        content: 'Claim your $500 gift card now by clicking this link!',
      },
      expected: {
        category: 'spam',
        response: "We're sorry, but your message has been automatically closed.",
      },
    },
    {
      input: {
        subject: 'FREE CA$H',
        content: 'BUY NOW ON WWW.BEST-DEALS.COM!',
      },
      expected: {
        category: 'spam',
        response: "We're sorry, but your message has been automatically closed.",
      },
    },
  ],
  
  // The task to run for each test case
  task: async ({ input }) => {
    return await classifyTicket(input);
  },
  
  // Scorers to measure performance
  scorers: [SpamClassificationScorer, ExactMatchScorer],
  
  // Optional metadata
  metadata: {
    description: 'Classify support tickets as spam or not spam',
  },
});
```

## Set up scorers

A scorer is a function that scores a capability’s output. Scorers receive the `input`, the generated `output`, and the `expected` value, and return a score (typically 0-1).

Examples to set up scorers:
- Use a simple exact match scorer that returns 1 if the output matches the expected value, and 0 otherwise:

    ```ts
    import { Scorer } from 'axiom/ai/evals';

    const ExactMatchScorer = Scorer(
      'Exact-Match',
      ({ output, expected }: { output: string; expected: string }) => {
        return output === expected ? 1 : 0;
      }
    );
    ```

- Use the `autoevals` library that provides prebuilt scorers for common tasks like semantic similarity, factual correctness, and text matching:

    ```ts
    import { Scorer } from 'axiom/ai/evals';
    import { ExactMatch } from 'autoevals';

    const WrappedExactMatch = Scorer(
      'Exact-Match',
      ({ output, expected }: { output: string; expected: string }) => {
        return ExactMatch({ output, expected });
      }
    );
    ```

- Use a custom scorer that returns metadata alongside the score:

    ```ts
    const CustomScorer = Scorer(
      'Custom-Scorer',
      ({ output, expected }) => {
        const score = computeScore(output, expected);
        return {
          score,
          metadata: {
            details: 'Additional info about this score',
          },
        };
      }
    );
    ```

## Run evaluations

To run your evaluation suites from your terminal, [install the Axiom CLI](/reference/cli) and use the following commands.

| Description | Command |
| ----------- | ------- |
| Run all evals | `axiom eval` |
| Run specific eval file | `axiom eval src/evals/ticket-classification.eval.ts` |
| Run evals matching a glob pattern | `axiom eval "**/*spam*.eval.ts"` |
| Run eval by name | `axiom eval "spam-classification"` |
| List available evals without running | `axiom eval --list` |

## Run experiments

Flags let you parameterize your AI behavior (like model choice or prompting strategies) and run experiments with different configurations. They’re type-safe via Zod schemas, and you can override them at runtime.

### Set up flags

Create the file `src/lib/app-scope.ts` that uses the `ticketClassification` flagto test different language models.

```ts /src/lib/app-scope.ts
import { createAppScope } from 'axiom/ai/evals';
import { z } from 'zod';

export const flagSchema = z.object({
  ticketClassification: z.object({
    model: z.string().default('gpt-4o-mini'),
  }),
});

const { flag, pickFlags } = createAppScope({ flagSchema });

export { flag, pickFlags };
```

### Override flags at runtime

Override flags directly when you run the eval:

```bash
axiom eval --flag.ticketClassification.model=gpt-4o
```

Alternatively, specify the flag overrides in a JSON file.

```json experiment.json
{
  "ticketClassification": {
    "model": "gpt-4o"
  }
}
```

And then specify the JSON file as the value of the `flags-config` parameter when you run the eval:

```bash
axiom eval --flags-config=experiment.json
```

## Analyze results in Console

<Badge>Coming soon</Badge> When you run an eval, Axiom AI SDK captures a detailed OpenTelemetry trace for the entire run. This includes parent spans for the evaluation suite and child spans for each individual test case, task execution, and scorer result. Axiom enriches the traces with `eval.*` attributes, allowing you to deeply analyze results in the Axiom Console.

The results of evals:
- Pass/fail status for each test case
- Scores from each scorer
- Comparison to baseline (if available)
- Links to view detailed traces in Axiom

Additionally, the AI SDK sends the results to your Axiom dataset for long-term tracking and analysis.

The Console will feature leaderboards and comparison views to track score progression across different versions of a capability, helping you verify that your changes are leading to measurable improvements.

## What’s next?

A capability is ready to be deployed when it meets your quality benchmarks. After deployment, the next steps can be the following:

- **Baseline comparisons**: Run evals multiple times to track regression over time.
- **Experiment with flags**: Test different models or strategies using flag overrides.
- **Advanced scorers**: Build custom scorers for domain-specific metrics.
- **CI/CD integration**: Add `axiom eval` to your CI pipeline to catch regressions.

The next step is to monitor your capability’s performance with real-world traffic. To learn more about this step of the AI engineering workflow, see [Observe](/ai-engineering/observe).
