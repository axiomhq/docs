---
title: "Quickstart"
description: "Install and configure Axiom AI SDK to begin capturing telemetry from your generative AI applications."
keywords: ["ai engineering", "getting started", "install", "setup", "configuration", "opentelemetry"]
---

import ReplaceDatasetToken from "/snippets/replace-dataset-token.mdx"
import ReplaceDomain from "/snippets/replace-domain.mdx"
import Prerequisites from "/snippets/standard-prerequisites.mdx"
import AIInstrumentationApproaches from "/snippets/ai-instrumentation-approaches.mdx"

Quickly start capturing telemetry data from your generative AI capabilities. After installation and configuration, follow the Axiom AI engineering workflow to create, evaluate, observe, and iterate.

<Tip>
This page explains how to set up instrumentation with Axiom AI SDK. Expand the section below to choose the right instrumentation approach for your needs.
</Tip>

<Accordion title="Choose your instrumentation approach">
<AIInstrumentationApproaches />
</Accordion>

<Prerequisites />

## Install

Install Axiom AI SDK into your TypeScript project:

<CodeGroup>

```bash pnpm
pnpm i axiom
```

```bash npm
npm i axiom
```

```bash yarn
yarn add axiom
```

```bash bun
bun add axiom
```

</CodeGroup>

<Info>
The `axiom` package includes the `axiom` command-line interface (CLI) for running evaluations, which you'll use to systematically test and improve your AI capabilities.
</Info>

## Configure tracer

To send data to Axiom, configure a tracer. For example, use a dedicated instrumentation file and load it before the rest of your app. An example configuration for a Node.js environment:

1. Install dependencies:

    <CodeGroup>

    ```bash pnpm
    pnpm i \
      dotenv \
      @opentelemetry/exporter-trace-otlp-http \
      @opentelemetry/resources \
      @opentelemetry/sdk-trace-node \
      @opentelemetry/semantic-conventions \
      @opentelemetry/api
    ```

    ```bash npm
    npm i \
      dotenv \
      @opentelemetry/exporter-trace-otlp-http \
      @opentelemetry/resources \
      @opentelemetry/sdk-trace-node \
      @opentelemetry/semantic-conventions \
      @opentelemetry/api
    ```

    ```bash yarn
    yarn add \
      dotenv \
      @opentelemetry/exporter-trace-otlp-http \
      @opentelemetry/resources \
      @opentelemetry/sdk-trace-node \
      @opentelemetry/semantic-conventions \
      @opentelemetry/api
    ```

    ```bash bun
    bun add \
      dotenv \
      @opentelemetry/exporter-trace-otlp-http \
      @opentelemetry/resources \
      @opentelemetry/sdk-trace-node \
      @opentelemetry/semantic-conventions \
      @opentelemetry/api
    ```

    </CodeGroup>

1. Create an instrumentation file:

    ```ts /src/instrumentation.ts expandable
    import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http';
    import { resourceFromAttributes } from '@opentelemetry/resources';
    import { BatchSpanProcessor, NodeTracerProvider } from '@opentelemetry/sdk-trace-node';
    import { ATTR_SERVICE_NAME } from '@opentelemetry/semantic-conventions';
    import { trace } from "@opentelemetry/api";
    import { initAxiomAI, RedactionPolicy } from 'axiom/ai';
    import type { AxiomEvalInstrumentationHook } from 'axiom/ai/config';

    const tracer = trace.getTracer("my-tracer");

    let provider: NodeTracerProvider | undefined;

    // Wrap your logic in the AxiomEvalInstrumentationHook function
    export const setupAppInstrumentation: AxiomEvalInstrumentationHook = async ({
      dataset,
      url,
      token,
    }) => {
      if (provider) {
        return { provider };
      }

      if (!dataset || !url || !token) {
        throw new Error('Missing environment variables');
      }

      // Replace the environment variables with the parameters passed to the function
      const exporter = new OTLPTraceExporter({
        url: `${url}/v1/traces`,
        headers: {
          Authorization: `Bearer ${token}`,
          'X-Axiom-Dataset': dataset,
        },
      })

      // Configure the provider to export traces to your Axiom dataset
      provider = new NodeTracerProvider({
        resource: resourceFromAttributes({
          [ATTR_SERVICE_NAME]: 'my-app', // Replace with your service name
        },
        {
          // Use the latest schema version
          // Info: https://opentelemetry.io/docs/specs/semconv/
          schemaUrl: 'https://opentelemetry.io/schemas/1.37.0',
        }),
        spanProcessor: new BatchSpanProcessor(exporter),
      });

      // Register the provider
      provider.register();

      // Initialize Axiom AI SDK with the configured tracer
      initAxiomAI({ tracer, redactionPolicy: RedactionPolicy.AxiomDefault });

      return { provider };
    };
    ```

For more information on specifying redaction policies, see [Redaction policies](/ai-engineering/redaction-policies).

### Create Axiom configuration file

The Axiom configuration file enables the evaluation framework, allowing you to run systematic tests against your AI capabilities and track improvements over time.

At the root of your project, create the Axiom configuration file `/axiom.config.ts`:

```ts /axiom.config.ts
import { defineConfig } from 'axiom/ai/config';
import { setupAppInstrumentation } from './src/instrumentation';

export default defineConfig({
  eval: {
    url: process.env.AXIOM_URL,
    token: process.env.AXIOM_TOKEN,
    dataset: process.env.AXIOM_DATASET,
    
    // Optional: customize which files to run
    include: ['**/*.eval.{ts,js}'],
    
    // Optional: exclude patterns
    exclude: [],
    
    // Optional: timeout for eval execution
    timeoutMs: 60_000,
    
    // Optional: instrumentation hook for OpenTelemetry
    // (created this in the "Create instrumentation setup" step)
    instrumentation: ({ url, token, dataset }) => 
      setupAppInstrumentation({ url, token, dataset }),
  },
});
```

## Store environment variables

Store environment variables in an `.env` file in the root of your project:

```bash .env
AXIOM_URL="AXIOM_DOMAIN"
AXIOM_TOKEN="API_TOKEN"
AXIOM_DATASET="DATASET_NAME"
OPENAI_API_KEY=""
GEMINI_API_KEY=""
XAI_API_KEY=""
ANTHROPIC_API_KEY=""
```

<Info>
<ReplaceDatasetToken />
<ReplaceDomain />

Enter the API keys for the LLMs you want to work with.
</Info>

<Note>
To run evaluations, you’ll need to authenticate the Axiom CLI. See [Setup and authentication](/ai-engineering/evaluate/setup) for details on using `axiom auth login`.
</Note>

## What’s next?

- **Build your first capability**: Start prototyping with [Create](/ai-engineering/create).
- **Set up evaluations**: Learn how to systematically test your capabilities with [Evaluate](/ai-engineering/evaluate/overview).
- **Capture production telemetry**: Instrument your AI calls for observability with [Observe](/ai-engineering/observe).
