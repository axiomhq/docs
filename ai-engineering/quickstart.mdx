---
title: "Quickstart"
description: "Install and configure Axiom AI SDK to begin capturing telemetry from your generative AI applications."
keywords: ["ai engineering", "getting started", "install", "setup", "configuration", "opentelemetry"]
---

import ReplaceDatasetToken from "/snippets/replace-dataset-token.mdx"
import Prerequisites from "/snippets/standard-prerequisites.mdx"
import AIInstrumentationApproaches from "/snippets/ai-instrumentation-approaches.mdx"

Quickly start capturing telemetry data from your generative AI apps. After installation and configuration, follow the Axiom AI engineering workflow to create, measure, observe, and iterate on your capabilities.

<Tip>
This page explains how to set up instrumentation with Axiom AI SDK. Expand the section below to chooose the right instrumentation approach for your needs.
</Tip>

<Accordion title="Choose your instrumentation approach">
<AIInstrumentationApproaches />
</Accordion>

<Prerequisites />

## Install

Install Axiom AI SDK into your TypeScript project:

<CodeGroup>

```bash pnpm
pnpm i axiom
```

```bash npm
npm i axiom
```

```bash yarn
yarn add axiom
```

```bash bun
bun add axiom
```

</CodeGroup>

<Info>
The `axiom` package includes the `axiom` command-line interface (CLI) for managing your AI assets, which will be used in later stages of the Axiom AI engineering workflow.
</Info>

## Configure tracer

To send data to Axiom, configure a tracer. For example, use a dedicated instrumentation file and load it before the rest of your app. An example configuration for a Node.js environment:

1. Install dependencies:

    <CodeGroup>

    ```bash pnpm
    pnpm i \
      dotenv \
      @opentelemetry/exporter-trace-otlp-http \
      @opentelemetry/resources \
      @opentelemetry/sdk-trace-node \
      @opentelemetry/semantic-conventions \
      @opentelemetry/api
    ```

    ```bash npm
    npm i \
      dotenv \
      @opentelemetry/exporter-trace-otlp-http \
      @opentelemetry/resources \
      @opentelemetry/sdk-trace-node \
      @opentelemetry/semantic-conventions \
      @opentelemetry/api
    ```

    ```bash yarn
    yarn add \
      dotenv \
      @opentelemetry/exporter-trace-otlp-http \
      @opentelemetry/resources \
      @opentelemetry/sdk-trace-node \
      @opentelemetry/semantic-conventions \
      @opentelemetry/api
    ```

    ```bash bun
    bun add \
      dotenv \
      @opentelemetry/exporter-trace-otlp-http \
      @opentelemetry/resources \
      @opentelemetry/sdk-trace-node \
      @opentelemetry/semantic-conventions \
      @opentelemetry/api
    ```

    </CodeGroup>

1. Create instrumentation file:

    ```typescript /src/instrumentation.ts

    import 'dotenv/config'; // Make sure to load environment variables
    import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http';
    import { resourceFromAttributes } from '@opentelemetry/resources';
    import { NodeTracerProvider } from '@opentelemetry/sdk-trace-node';
    import { SimpleSpanProcessor } from '@opentelemetry/sdk-trace-node';
    import { ATTR_SERVICE_NAME } from '@opentelemetry/semantic-conventions';
    import { trace } from "@opentelemetry/api";
    import { initAxiomAI, RedactionPolicy } from 'axiom/ai';

    const tracer = trace.getTracer("my-tracer");

    // Configure the provider to export traces to your Axiom dataset
    const provider = new NodeTracerProvider({
      resource: resourceFromAttributes({
        [ATTR_SERVICE_NAME]: 'my-ai-app', // Replace with your service name
      },
      {
        // Use the latest schema version
        // Info: https://opentelemetry.io/docs/specs/semconv/
        schemaUrl: 'https://opentelemetry.io/schemas/1.37.0',
      }),
      spanProcessor: new SimpleSpanProcessor(
        new OTLPTraceExporter({
          url: `https://api.axiom.co/v1/traces`,
          headers: {
            Authorization: `Bearer ${process.env.AXIOM_TOKEN}`,
            'X-Axiom-Dataset': process.env.AXIOM_DATASET!,
          },
        })
      ),
    });

    // Register the provider
    provider.register();

    // Initialize Axiom AI SDK with the configured tracer
    initAxiomAI({ tracer, redactionPolicy: RedactionPolicy.AxiomDefault });
    ```

For more information on specifying redaction policies, see [Redaction policies](/ai-engineering/redaction-policies).

## Store environment variables

Store environment variables in an `.env` file in the root of your project:

```bash .env
AXIOM_TOKEN="API_TOKEN"
AXIOM_DATASET="DATASET_NAME"
OPENAI_API_KEY=""
GEMINI_API_KEY=""
XAI_API_KEY=""
ANTHROPIC_API_KEY=""
```

<Info>
<ReplaceDatasetToken />

Enter the API keys for the LLMs you want to work with.
</Info>

## Whatâ€™s next?

- **Explore the AI engineering workflow**: Start building systematic AI capabilities beginning with [Create](/ai-engineering/create).
- **Continue with Axiom AI SDK**: Learn about instrumenting your AI model and tool calls in [Observe](/ai-engineering/observe).
