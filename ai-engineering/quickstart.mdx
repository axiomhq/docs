---
title: "Quickstart"
description: "Install and configure the Axiom AI SDK to begin capturing telemetry from your generative AI applications."
keywords: ["ai engineering", "getting started", "install", "setup", "configuration", "opentelemetry"]
---

import AIEngineeringInstrumentationSnippet from '/snippets/ai-engineering-instrumentation.mdx'
import ReplaceDatasetToken from "/snippets/replace-dataset-token.mdx"

This guide provides the steps to install and configure Axiom’s [AI SDK](https://github.com/axiomhq/ai). Once configured, you can follow the Rudder workflow to create, measure, observe, and iterate on your capabilities.

## Prerequisites

Before you begin, ensure you have the following:

* An Axiom **account**. Create one [here](https://www.axiom.co/register).
* An Axiom **dataset**. Create one [here](https://app.axiom.co/datasets).
* An Axiom **API token**. Create one [here](https://app.axiom.co/settings/api-tokens).

## Installation

Install the Axiom AI SDK into your TypeScript project using your preferred package manager.

<CodeGroup>

```bash pnpm
pnpm i axiom
```

```bash npm
npm i axiom
```

```bash yarn
yarn add axiom
```

```bash bun
bun add axiom
```

</CodeGroup>

The SDK is open source. You can view the source code and examples in Axiom’s [AI SDK](https://github.com/axiomhq/ai) GitHub repository.

<Tip>
The `axiom` package includes the `axiom` command-line interface (CLI) for managing your AI assets, which will be used in later stages of the Rudder workflow.
</Tip>

## Configuration

Axiom’s AI SDK is built on the OpenTelemetry standard and requires a configured tracer to send data to Axiom. This is typically done in a dedicated instrumentation file that’s loaded before the rest of your application.

Here is a standard configuration for a Node.js environment:

<CodeGroup>

```bash pnpm
pnpm i \
  dotenv \
  @opentelemetry/exporter-trace-otlp-http \
  @opentelemetry/resources \
  @opentelemetry/sdk-node \
  @opentelemetry/sdk-trace-node \
  @opentelemetry/semantic-conventions \
  @opentelemetry/api
```

```bash npm
npm i \
  dotenv \
  @opentelemetry/exporter-trace-otlp-http \
  @opentelemetry/resources \
  @opentelemetry/sdk-node \
  @opentelemetry/sdk-trace-node \
  @opentelemetry/semantic-conventions \
  @opentelemetry/api
```

```bash yarn
yarn add \
  dotenv \
  @opentelemetry/exporter-trace-otlp-http \
  @opentelemetry/resources \
  @opentelemetry/sdk-node \
  @opentelemetry/sdk-trace-node \
  @opentelemetry/semantic-conventions \
  @opentelemetry/api
```

```bash bun
bun add \
  dotenv \
  @opentelemetry/exporter-trace-otlp-http \
  @opentelemetry/resources \
  @opentelemetry/sdk-node \
  @opentelemetry/sdk-trace-node \
  @opentelemetry/semantic-conventions \
  @opentelemetry/api
```

</CodeGroup>

<AIEngineeringInstrumentationSnippet />

## Environment variables

Your Axiom credentials and any frontier model API keys should be stored as environment variables. Create a `.env` file in the root of your project:

```bash
# Axiom Credentials
AXIOM_TOKEN="API_TOKEN"
AXIOM_DATASET="DATASET_NAME"

# Frontier Model API Keys
OPENAI_API_KEY="OPENAI_API_KEY"
GEMINI_API_KEY="GEMINI_API_KEY"
```

<Info>
<ReplaceDatasetToken />
</Info>

## What’s next?

Now that your application is configured to send telemetry to Axiom, the next step is to start instrumenting your AI model and tool calls.

Learn more about that in the [Observe](/ai-engineering/observe) page of the Rudder workflow.