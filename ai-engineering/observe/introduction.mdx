---
title: "Introduction"
description: "Learn how to observe your deployed AI capabilities in production using Axiom's AI SDK to capture telemetry."
keywords: ["ai engineering", "rudder", "observe", "telemetry", "withspan", "opentelemetry"]
---

import { Badge } from "/snippets/badge.jsx"

The **Observe** stage is about understanding how your deployed generative AI capabilities perform in the real world. After creating and evaluating a capability, observing its production behavior is crucial for identifying unexpected issues, tracking costs, and gathering the data needed for future improvements.

## Choose your instrumentation approach

Your observability setup depends on the integration approach you selected in the [Quickstart](/ai-engineering/quickstart):

<CardGroup cols={2}>
  <Card title="Axiom AI SDK" icon="bolt" href="/ai-engineering/observe/typescript-sdk">
    **Fastest setup** - Use our wrapper around the Vercel AI SDK for automatic instrumentation with minimal code changes. See the TypeScript SDK guide.
  </Card>
  <Card title="Custom OpenTelemetry" icon="code" href="/ai-engineering/observe/manual-instrumentation">
    **Full control** - Send traces using your own tooling following our semantic conventions.
  </Card>
</CardGroup>

## Understanding your AI telemetry

Once instrumented, every LLM call will send a detailed span to your Axiom dataset. These spans are enriched with standardized `gen_ai.*` attributes that make your AI interactions easy to query and analyze.

Key attributes include:

* `gen_ai.capability.name`: The high-level capability name you defined in `withSpan`.
* `gen_ai.step.name`: The specific step within the capability.
* `gen_ai.request.model`: The model requested for the completion.
* `gen_ai.response.model`: The model that actually fulfilled the request.
* `gen_ai.usage.input_tokens`: The number of tokens in the prompt.
* `gen_ai.usage.output_tokens`: The number of tokens in the generated response.
* `gen_ai.prompt`: The full, rendered prompt or message history sent to the model (as a JSON string).
* `gen_ai.completion`: The full response from the model, including tool calls (as a JSON string).
* `gen_ai.response.finish_reasons`: The reason the model stopped generating tokens (e.g., `stop`, `tool-calls`).
* **`gen_ai.tool.name`**: The name of the executed tool.
* **`gen_ai.tool.arguments`**: The arguments passed to the tool (as a JSON string).
* **`gen_ai.tool.message`**: The result returned by the tool (as a JSON string).

## Visualizing traces in the console

Visualizing and making sense of this telemetry data is a core part of the Axiom Console experience.

* A dedicated **AI Trace Waterfall** view will visualize single and multi-step LLM workflows, with clear input/output inspection at each stage.
* A pre-built **Gen AI OTel Dashboard** will automatically appear for any dataset receiving AI telemetry. It will feature elements for tracking cost per invocation, time-to-first-token, call counts by model, and error rates.

## What's next?

Now that you are capturing and analyzing production telemetry, the next step is to use these insights to improve your capability.

Learn more in the [Iterate](/ai-engineering/iterate) page.
