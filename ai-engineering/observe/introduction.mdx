---
title: "Introduction"
description: "Learn how to observe your deployed AI capabilities in production using Axiom AI SDK to capture telemetry."
keywords: ["ai engineering", "rudder", "observe", "telemetry", "withspan", "opentelemetry"]
---

import { Badge } from "/snippets/badge.jsx"
import AIInstrumentationApproaches from "/snippets/ai-instrumentation-approaches.mdx"

In the Observe stage of the AI engineering lifecycle, the focus is on understanding how your deployed generative AI capabilities perform in the real world. After creating and evaluating a capability, observing its production behavior is crucial for identifying unexpected issues, tracking costs, and gathering the data needed for future improvements.

## Instrumentation approaches

<AIInstrumentationApproaches />

## Understand AI telemetry

After you instrument your app, every LLM call sends a detailed span to your Axiom dataset. The spans are enriched with standardized `gen_ai.*` attributes that make your AI interactions easy to query and analyze.

Key attributes include:

- `gen_ai.capability.name`: The high-level capability name you defined in `withSpan`.
- `gen_ai.step.name`: The specific step within the capability.
- `gen_ai.request.model`: The model requested for the completion.
- `gen_ai.response.model`: The model that actually fulfilled the request.
- `gen_ai.usage.input_tokens`: The number of tokens in the prompt.
- `gen_ai.usage.output_tokens`: The number of tokens in the generated response.
- `gen_ai.prompt`: The full, rendered prompt or message history sent to the model (as a JSON string).
- `gen_ai.completion`: The full response from the model, including tool calls (as a JSON string).
- `gen_ai.response.finish_reasons`: The reason the model stopped generating tokens. For example: `stop`, `tool-calls`.
- `gen_ai.tool.name`: The name of the executed tool.
- `gen_ai.tool.arguments`: The arguments passed to the tool (as a JSON string).
- `gen_ai.tool.message`: The result returned by the tool (as a JSON string).

## Visualize traces in Console

Visualizing and making sense of this telemetry data is a core part of the Axiom Console experience:

- A dedicated **AI Trace Waterfall** view visualizes single and multi-step LLM workflows, with clear input/output inspection at each stage.
- A pre-built **Gen AI OTel Dashboard** automatically appears for any dataset receiving AI telemetry. It features elements for tracking cost per invocation, time-to-first-token, call counts by model, and error rates.

## Whatâ€™s next?

Now that you are capturing and analyzing production telemetry, the next step is to use these insights to improve your capability. Learn more in [Iterate](/ai-engineering/iterate).
